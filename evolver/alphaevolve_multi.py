# evolver/alphaevolve_multi.py
"""
Data-Aware AlphaEvolve (HF dataset, ICL-only baseline, OpenRouter).

- ç»“æ„åŒ– Prompt è¿›åŒ–ï¼šPromptGenome + GA
- Few-shot ICL ç¤ºä¾‹éšåŸºå› ä¸€èµ·è¿›åŒ–ï¼ˆicl_strategy, k_shotsï¼‰
- å½“å‰é˜¶æ®µ RAG=none, summary=false, teacher=falseï¼ˆbaselineï¼‰
- å¤šæŒ‡æ ‡è¯„ä¼°ï¼šQWK / Pearson / RMSE / Accuracy
  * ä¼˜å…ˆçº§ï¼šQWK > Pearson > RMSE
- âœ… æ–°å¢ï¼šæ¯ä»£ç»Ÿè®¡ LLM text/ICL å˜å¼‚è§¦å‘ç‡ã€æˆåŠŸç‡
"""

from __future__ import annotations

import json
import math
import os
import random
import time
import copy
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

from evolver.prompt_evolver import (
    Individual,
    build_initial_population,
    tournament_selection,
    crossover_genome,
    mutate_genome,
    set_llm_feedback,
    load_template_pool,
    update_template_pool,
    reset_llm_stats,   # âœ… æ–°å¢
    get_llm_stats,     # âœ… æ–°å¢
)
from evolver.data_aware_prompt import PromptGenome, build_full_prompt, INSTRUCTION_TEMPLATES
from evolver.icl_sampler import select_icl_examples
from evolver.checkpoint import (  # ğŸ”¥ æ–°å¢ï¼šæ–­ç‚¹ç»­ä¼ 
    save_checkpoint,
    load_checkpoint,
    restore_population,
    restore_best_individual,
    clean_old_checkpoints,
)
from llm_api.openrouter_api import call_scoring_llm

# ================== è·¯å¾„ & å¸¸é‡é…ç½® ================== #

BASE_DIR = Path(__file__).resolve().parents[1]

RAW_HF_TRAIN = BASE_DIR / "data" / "raw" / "hf_dataset" / "train.csv"
PROC_DIR = BASE_DIR / "data" / "processed" / "hf_dataset"
TRAIN_CLEAN = PROC_DIR / "train_clean.csv"
EVAL_CLEAN = PROC_DIR / "eval_clean.csv"

LOG_DIR = BASE_DIR / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)

BEST_JSON = LOG_DIR / "best_scoring_prompt_hf.json"
BEST_TXT = LOG_DIR / "best_scoring_prompt_hf.txt"
BEST_PRED_CSV = LOG_DIR / "best_prompt_predictions_hf.csv"
METRIC_FIG = LOG_DIR / "metrics_curve_hf.png"

# âœ… æ¨¡æ¿æ± æŒä¹…åŒ–è·¯å¾„
TEMPLATE_POOL_JSON = LOG_DIR / "template_pool.json"

# ğŸ”¥ æ–­ç‚¹ç»­ä¼ é…ç½®
CHECKPOINT_DIR = LOG_DIR / "checkpoints"
CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)
ENABLE_CHECKPOINT = os.getenv("ENABLE_CHECKPOINT", "1") == "1"
CHECKPOINT_EVERY_GEN = int(os.getenv("CHECKPOINT_EVERY_GEN", "1"))  # æ¯ N ä»£ä¿å­˜ä¸€æ¬¡

# -------- GA è¶…å‚æ•° --------
POP_SIZE = 10
N_GENERATIONS = 6
TOURNAMENT_K = 4
CROSSOVER_RATE = 0.85
MUTATION_RATE = 0.35

# ğŸ”¥ åˆ†é˜¶æ®µè¯„ä¼°é…ç½®
USE_STAGED_EVAL = os.getenv("USE_STAGED_EVAL", "0") == "1"
N_EVAL_SAMPLES = int(os.getenv("N_EVAL_SAMPLES", "64"))
N_EVAL_SAMPLES_EARLY = int(os.getenv("N_EVAL_SAMPLES_EARLY", "32"))
N_EVAL_SAMPLES_LATE = int(os.getenv("N_EVAL_SAMPLES_LATE", "64"))
EARLY_PHASE_RATIO = float(os.getenv("EARLY_PHASE_RATIO", "0.67"))

MAX_CONTEXT_CHARS = 12000
EARLYSTOP_CONSEC_FAIL = 3
EARLYSTOP_FAIL_RATE = 0.6
MIN_SAMPLES_BEFORE_EARLYSTOP = 8

SINGLE_MODEL = os.getenv("OPENROUTER_MODEL", "meta-llama/llama-3.3-70b-instruct")

# å¯é…ç½®çš„æœ€å°è°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
MIN_INTERVAL = float(os.getenv("OPENROUTER_MIN_INTERVAL", "1.0"))
_last_call_ts = 0.0


def throttle():
    global _last_call_ts
    now = time.time()
    wait = MIN_INTERVAL - (now - _last_call_ts)
    if wait > 0:
        time.sleep(wait)
    _last_call_ts = time.time()


_LLM_CACHE: Dict[str, Optional[str]] = {}


def log(msg: str) -> None:
    print(msg, flush=True)


def parse_band_from_text(text: str, default: float = 5.0) -> float:
    import re
    if not text:
        return default
    clean = text.replace(",", " ").replace("\n", " ")
    nums = re.findall(r"(?<!\d)([0-9](?:\.5)?)(?!\d)", clean)
    if not nums:
        nums = re.findall(r"\d+(?:\.\d+)?", clean)
        if not nums:
            return default
    val = float(nums[0])
    val = max(0.0, min(9.0, val))
    return round(val * 2) / 2.0


def quadratic_weighted_kappa(y_true: List[float], y_pred: List[float]) -> float:
    y_true = np.array(y_true, dtype=float)
    y_pred = np.array(y_pred, dtype=float)
    y_true_i = np.round(y_true * 2).astype(int)
    y_pred_i = np.round(y_pred * 2).astype(int)

    min_rating, max_rating = 0, 18
    n_ratings = max_rating - min_rating + 1

    O = np.zeros((n_ratings, n_ratings), dtype=float)
    for a, b in zip(y_true_i, y_pred_i):
        if 0 <= a <= max_rating and 0 <= b <= max_rating:
            O[a, b] += 1.0

    hist_true = np.zeros(n_ratings, dtype=float)
    hist_pred = np.zeros(n_ratings, dtype=float)
    for a in y_true_i:
        if 0 <= a <= max_rating:
            hist_true[a] += 1.0
    for b in y_pred_i:
        if 0 <= b <= max_rating:
            hist_pred[b] += 1.0

    E = np.outer(hist_true, hist_pred)
    if E.sum() == 0:
        return 0.0
    E = E / E.sum() * O.sum()

    W = np.zeros((n_ratings, n_ratings), dtype=float)
    for i in range(n_ratings):
        for j in range(n_ratings):
            W[i, j] = ((i - j) ** 2) / ((max_rating - min_rating) ** 2)

    num = (W * O).sum()
    den = (W * E).sum()
    if den == 0:
        return 0.0
    return 1.0 - num / den


def compute_metrics(y_true: List[float], y_pred: List[float]) -> Dict[str, float]:
    y_true_arr = np.array(y_true, dtype=float)
    y_pred_arr = np.array(y_pred, dtype=float)

    qwk = quadratic_weighted_kappa(y_true_arr.tolist(), y_pred_arr.tolist())

    try:
        if np.std(y_pred_arr) == 0 or np.std(y_true_arr) == 0:
            pearson = 0.0
        else:
            pearson = float(pearsonr(y_true_arr, y_pred_arr)[0])
    except Exception:
        pearson = 0.0

    rmse = float(np.sqrt(np.mean((y_true_arr - y_pred_arr) ** 2)))

    exact_acc = float(np.mean(y_true_arr == y_pred_arr))
    adj_acc = float(np.mean(np.abs(y_true_arr - y_pred_arr) <= 0.5))

    return {
        "qwk": qwk,
        "pearson": pearson,
        "rmse": rmse,
        "exact_acc": exact_acc,
        "adj_acc": adj_acc,
    }


def fitness_from_metrics(m: Dict[str, float]) -> float:
    qwk = m["qwk"]
    pearson = max(m["pearson"], 0.0)
    rmse = m["rmse"]
    return qwk + 0.3 * pearson - 0.2 * rmse


# ================== åå·®ç»Ÿè®¡ï¼ˆç»™ LLM åé¦ˆç”¨ï¼‰================== #

def compute_bias_stats(labels: List[float], preds: List[float]) -> Dict[str, Any]:
    """ç»Ÿè®¡ pred-true çš„åå·®ï¼šæ•´ä½“ + åˆ† bandã€‚"""
    if not labels:
        return {}

    labels_arr = np.array(labels, dtype=float)
    preds_arr = np.array(preds, dtype=float)
    err = preds_arr - labels_arr

    mean_err = float(np.mean(err))
    mae = float(np.mean(np.abs(err)))
    over_rate = float(np.mean(err > 0))
    under_rate = float(np.mean(err < 0))

    by_band: Dict[int, Dict[str, Any]] = {}
    for b in sorted(set(np.round(labels_arr * 2).astype(int))):
        mask = (np.round(labels_arr * 2).astype(int) == b)
        e = err[mask]
        if e.size == 0:
            continue
        by_band[int(b)] = {
            "n": int(e.size),
            "mean_err": float(np.mean(e)),
            "mae": float(np.mean(np.abs(e))),
        }

    return {
        "mean_err": mean_err,
        "mae": mae,
        "over_rate": over_rate,
        "under_rate": under_rate,
        "by_band": by_band,
    }


# ================== HF æ•°æ®åŠ è½½ ================== #

Sample = Tuple[int, str, float, str]

def _safe_float(x: Any) -> Optional[float]:
    try:
        return float(str(x).strip())
    except Exception:
        return None


def load_hf_dataset() -> Tuple[List[Sample], List[Sample]]:
    if TRAIN_CLEAN.exists() and EVAL_CLEAN.exists():
        train_df = pd.read_csv(TRAIN_CLEAN)
        eval_df = pd.read_csv(EVAL_CLEAN)
        log(f"Loaded HF train_clean: {len(train_df)} samples.")
        log(f"Loaded HF eval_clean : {len(eval_df)} samples.")
    else:
        df = pd.read_csv(RAW_HF_TRAIN)
        df = df.dropna(subset=["essay", "band"]).reset_index(drop=True)
        train_df = df
        eval_df = df.sample(frac=0.05, random_state=42)
        train_df = df.drop(eval_df.index).reset_index(drop=True)
        eval_df = eval_df.reset_index(drop=True)
        log(f"Loaded RAW HF train: {len(train_df)} | eval: {len(eval_df)}")

    def to_samples(d: pd.DataFrame) -> List[Sample]:
        out: List[Sample] = []
        for i, row in d.iterrows():
            essay = str(row.get("essay", "")).strip()
            prompt = str(row.get("prompt", "")).strip()
            band = _safe_float(row.get("band"))
            if band is None:
                continue
            out.append((i, essay, float(band), prompt))
        return out

    return to_samples(train_df), to_samples(eval_df)


# ================== RAG (stub) ================== #

class DummyRAG:
    def retrieve(self, essay: str, strategy: str = "none", k: int = 3):
        return []


# ================== ä¸ªä½“è¯„ä¼° ================== #

def evaluate_individual(
    ind: Individual,
    eval_pool: List[Sample],
    train_pool: List[Sample],
    rag: DummyRAG,
) -> Dict[str, float]:
    preds: List[float] = []
    labels: List[float] = []

    consecutive_fail = 0
    fail_cnt = 0
    valid_cnt = 0

    MIN_VALID_RATIO = 0.6
    MAX_FAIL_STREAK = EARLYSTOP_CONSEC_FAIL

    for i, (sid, essay, true_band, prompt_text) in enumerate(eval_pool, start=1):

        # ğŸ”¥ æ ¹æ®æ¨¡å¼é€‰æ‹© ICL ç¤ºä¾‹
        if ind.genome.use_icl_indices and ind.genome.icl_indices:
            # æ–°æ¨¡å¼ï¼šä½¿ç”¨ç´¢å¼•åˆ—è¡¨
            icl_examples = select_icl_examples(
                train_pool,
                strategy="random",  # ä¸ä½¿ç”¨
                k=ind.genome.k_shots,
                seed=sid,
                indices=ind.genome.icl_indices,
            )
        else:
            # æ—§æ¨¡å¼ï¼šä½¿ç”¨ç­–ç•¥
            icl_examples = select_icl_examples(
                train_pool,
                strategy=ind.genome.icl_strategy,
                k=ind.genome.k_shots,
                seed=sid,
            )

        rag_examples = rag.retrieve(
            essay,
            strategy=ind.genome.rag_strategy,
            k=3,
        )

        full_prompt = build_full_prompt(
            genome=ind.genome,
            essay=essay,
            icl_examples=icl_examples,
            rag_examples=rag_examples,
            summary_text=None,
        )

        if len(full_prompt) > MAX_CONTEXT_CHARS:
            head = full_prompt[: int(MAX_CONTEXT_CHARS * 0.75)]
            tail = full_prompt[-int(MAX_CONTEXT_CHARS * 0.20):]
            full_prompt = head + "\n\n[Truncated]\n\n" + tail

        cache_key = full_prompt
        cached = _LLM_CACHE.get(cache_key)

        if cached is None:
            throttle()
            reply = call_scoring_llm(
                full_prompt,
                temperature=0.0,
                model=SINGLE_MODEL,
                max_tokens=8,
                max_retries=3,
                timeout=60,
            )
            _LLM_CACHE[cache_key] = reply
        else:
            reply = cached

        if not reply:
            consecutive_fail += 1
            fail_cnt += 1
            raw_band = 5.0
        else:
            consecutive_fail = 0
            raw_band = parse_band_from_text(reply, default=5.0)
            valid_cnt += 1

        calibrated = round(float(raw_band) * 2) / 2.0
        preds.append(calibrated)
        labels.append(true_band)

        log(
            f"  [{i}/{len(eval_pool)}] "
            f"True={true_band:.1f} | Raw={raw_band:.1f} | Pred={calibrated:.1f}"
        )

        cur_fail_rate = fail_cnt / i
        if consecutive_fail >= MAX_FAIL_STREAK:
            log(f"  [EarlyStop] consecutive_fail={consecutive_fail} â†’ stop this individual.")
            break

        if i >= MIN_SAMPLES_BEFORE_EARLYSTOP and cur_fail_rate >= EARLYSTOP_FAIL_RATE:
            log(f"  [EarlyStop] fail_rate={cur_fail_rate:.2%} â‰¥ {EARLYSTOP_FAIL_RATE:.0%} â†’ stop.")
            break

        if i >= MIN_SAMPLES_BEFORE_EARLYSTOP:
            remain = len(eval_pool) - i
            if (valid_cnt + remain) / len(eval_pool) < MIN_VALID_RATIO:
                log(f"  [EarlyStop] valid_cnt too low â†’ stop.")
                break

    m = compute_metrics(labels, preds)

    if valid_cnt < MIN_VALID_RATIO * len(labels):
        log(f"  [LowValid] valid={valid_cnt}/{len(labels)} â†’ force low fitness.")
        ind.fitness = -1e9
    else:
        ind.fitness = fitness_from_metrics(m)

    ind.metrics = m
    ind.preds = preds
    ind.labels = labels

    log(
        f"  â†’ QWK={m['qwk']:.4f}, Pearson={m['pearson']:.4f}, "
        f"RMSE={m['rmse']:.4f}, Exact={m['exact_acc']:.3f}, "
        f"Adj={m['adj_acc']:.3f}, Fitness={ind.fitness:.4f}, "
        f"FailRate={fail_cnt}/{len(labels)}"
    )
    return m


def get_eval_pool_size(gen: int, total_gens: int) -> int:
    """
    æ ¹æ®ä»£æ•°è¿”å›è¯„ä¼°æ ·æœ¬æ•°ã€‚
    
    å¦‚æœå¯ç”¨åˆ†é˜¶æ®µè¯„ä¼°ï¼š
    - å‰æœŸï¼ˆå‰ 67% ä»£æ•°ï¼‰ï¼šä½¿ç”¨è¾ƒå°‘æ ·æœ¬å¿«é€Ÿç­›é€‰
    - åæœŸï¼ˆå 33% ä»£æ•°ï¼‰ï¼šä½¿ç”¨è¾ƒå¤šæ ·æœ¬ç²¾ç¡®è¯„ä¼°
    
    Args:
        gen: å½“å‰ä»£æ•°
        total_gens: æ€»ä»£æ•°
    
    Returns:
        è¯„ä¼°æ ·æœ¬æ•°
    """
    if not USE_STAGED_EVAL:
        return N_EVAL_SAMPLES
    
    if gen <= total_gens * EARLY_PHASE_RATIO:
        return N_EVAL_SAMPLES_EARLY
    else:
        return N_EVAL_SAMPLES_LATE


def stratified_sample(eval_pool_full, n=32, seed=42):
    rng = random.Random(seed)
    buckets = {}
    for s in eval_pool_full:
        key = int(round(s[2] * 2))
        buckets.setdefault(key, []).append(s)

    all_keys = sorted(buckets.keys())
    total = len(eval_pool_full)
    out = []

    for k in all_keys:
        frac = len(buckets[k]) / total
        take = max(1, round(frac * n))
        out.extend(rng.sample(buckets[k], min(take, len(buckets[k]))))

    if len(out) < n:
        rest = [s for s in eval_pool_full if s not in out]
        out.extend(rng.sample(rest, n - len(out)))

    if len(out) > n:
        out = rng.sample(out, n)

    rng.shuffle(out)
    return out


# ================== GA ä¸»æµç¨‹ ================== #

def run_evolution_hf_icl_only():
    print("==== Data-Aware AlphaEvolve (ICL-only baseline, OpenRouter) ====")
    print(f"Single model: {SINGLE_MODEL}")
    print(f"Checkpoint enabled: {ENABLE_CHECKPOINT}")

    # âœ… å¯åŠ¨æ—¶åŠ è½½æ¨¡æ¿æ± 
    load_template_pool(TEMPLATE_POOL_JSON)

    train_pool, eval_pool_full = load_hf_dataset()
    
    print(f"Train pool: {len(train_pool)} | Eval pool (full): {len(eval_pool_full)}")
    
    if USE_STAGED_EVAL:
        print(f"ğŸ¯ Staged evaluation enabled:")
        print(f"   Early phase (gen 1-{int(N_GENERATIONS * EARLY_PHASE_RATIO)}): {N_EVAL_SAMPLES_EARLY} samples")
        print(f"   Late phase (gen {int(N_GENERATIONS * EARLY_PHASE_RATIO)+1}-{N_GENERATIONS}): {N_EVAL_SAMPLES_LATE} samples")
    else:
        print(f"ğŸ“Š Fixed evaluation: {N_EVAL_SAMPLES} samples per generation")

    # ğŸ”¥ å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
    checkpoint = None
    if ENABLE_CHECKPOINT:
        latest_checkpoint = CHECKPOINT_DIR / "checkpoint_latest.json"
        checkpoint = load_checkpoint(latest_checkpoint)
    
    # åˆå§‹åŒ–æˆ–æ¢å¤çŠ¶æ€
    if checkpoint is not None:
        print("\nğŸ”„ Resuming from checkpoint...")
        start_gen = checkpoint["generation"] + 1
        population = restore_population(checkpoint["population"])
        best_overall_ind = restore_best_individual(checkpoint["best_overall"])
        best_overall_fitness = checkpoint["best_overall_fitness"]
        history_qwk = checkpoint["history"]["qwk"]
        history_pearson = checkpoint["history"]["pearson"]
        history_rmse = checkpoint["history"]["rmse"]
        history_llm_stats = checkpoint["history"]["llm_stats"]
        
        # æ¢å¤ LLM ç¼“å­˜
        global _LLM_CACHE
        _LLM_CACHE = checkpoint.get("llm_cache", {})
        print(f"   Restored {len(_LLM_CACHE)} cached LLM calls")
        print(f"   Starting from generation {start_gen}")
    else:
        print("\nğŸ†• Starting fresh evolution...")
        start_gen = 1
        population = build_initial_population(
            pop_size=POP_SIZE,
            train_pool_size=len(train_pool)
        )
        history_qwk, history_pearson, history_rmse = [], [], []
        history_llm_stats: List[Dict[str, Any]] = []
        best_overall_ind: Optional[Individual] = None
        best_overall_fitness = -math.inf

    rag = DummyRAG()

    for gen in range(start_gen, N_GENERATIONS + 1):
        print(f"\n=== Generation {gen}/{N_GENERATIONS} ===")

        # âœ… æ¯ä»£å¼€å§‹æ¸…ç©º LLM ç»Ÿè®¡ï¼šç»Ÿè®¡â€œæœ¬ä»£äº§ç”Ÿä¸‹ä¸€ä»£æ—¶â€çš„ LLM è´¡çŒ®
        reset_llm_stats()

        gen_best_ind: Optional[Individual] = None
        gen_best_metrics: Optional[Dict[str, float]] = None
        gen_best_fitness = -math.inf

        # ====== è¯„ä¼°æœ¬ä»£æ‰€æœ‰ä¸ªä½“ ======
        for i, ind in enumerate(population, start=1):
            print(f"\n[Gen {gen}] Individual {i}/{len(population)}")
            print(f"Genome: {ind.genome}")

            metrics = evaluate_individual(ind, eval_pool, train_pool, rag)
            fit = ind.fitness if ind.fitness is not None else -math.inf

            if fit > gen_best_fitness:
                gen_best_fitness = fit
                gen_best_ind = copy.deepcopy(ind)  # âœ… ä¿ç•™ labels/preds/metrics
                gen_best_metrics = metrics

        assert gen_best_ind is not None and gen_best_metrics is not None

        history_qwk.append(gen_best_metrics["qwk"])
        history_pearson.append(gen_best_metrics["pearson"])
        history_rmse.append(gen_best_metrics["rmse"])

        print(
            f"\n[Gen {gen}] Best in generation: "
            f"QWK={gen_best_metrics['qwk']:.4f}, "
            f"Pearson={gen_best_metrics['pearson']:.4f}, "
            f"RMSE={gen_best_metrics['rmse']:.4f}, "
            f"Fitness={gen_best_fitness:.4f}"
        )

        # âœ… æ›´æ–° overall best
        if gen_best_fitness > best_overall_fitness:
            best_overall_fitness = gen_best_fitness
            best_overall_ind = copy.deepcopy(gen_best_ind)

        # âœ… è®¡ç®—åå·®ç»Ÿè®¡ + å–‚ç»™ LLM
        best_text = (
            gen_best_ind.genome.instruction_text
            or INSTRUCTION_TEMPLATES.get(
                gen_best_ind.genome.instruction_id, INSTRUCTION_TEMPLATES[0]
            )
        )
        bias_stats = compute_bias_stats(gen_best_ind.labels or [], gen_best_ind.preds or [])
        set_llm_feedback(best_text, bias_stats, gen_best_metrics, gen)

        # âœ… æŠŠå¥½æ¨¡æ¿å†™è¿›æ¨¡æ¿æ± å¹¶æŒä¹…åŒ–
        update_template_pool(best_text, gen_best_fitness, gen_best_metrics, gen)

        # ====== äº§ç”Ÿä¸‹ä¸€ä»£ï¼ˆæœ€åä¸€ä»£ä¸éœ€è¦ç”Ÿæˆï¼‰ ======
        if gen < N_GENERATIONS:
            parents = tournament_selection(
                population, k=TOURNAMENT_K, num_winners=POP_SIZE
            )
            new_population: List[Individual] = []
            # elitism: ä¿ç•™æœ¬ä»£æœ€ä¼˜
            new_population.append(
                Individual(genome=copy.deepcopy(gen_best_ind.genome))
            )

            rng = random.Random(gen * 999)
            while len(new_population) < POP_SIZE:
                p1, p2 = rng.sample(parents, 2)
                child_genome = copy.deepcopy(p1.genome)

                if rng.random() < CROSSOVER_RATE:
                    child_genome = crossover_genome(p1.genome, p2.genome, rng)

                child_genome = mutate_genome(
                    child_genome, 
                    mutation_rate=MUTATION_RATE, 
                    rng=rng,
                    train_pool_size=len(train_pool)
                )
                new_population.append(Individual(genome=child_genome))

            population = new_population

        # âœ… å˜å¼‚éƒ½å‘ç”Ÿå®Œäº†ï¼ˆæˆ–æœ€åä¸€ä»£æ— å˜å¼‚ï¼‰ï¼Œå†ç»Ÿè®¡/æ‰“å°
        stats = get_llm_stats()
        history_llm_stats.append({"gen": gen, **stats})
        print(f"\n[Gen {gen}] LLM mutation stats:")
        print(json.dumps(stats, ensure_ascii=False, indent=2))
        
        # ğŸ”¥ ä¿å­˜æ£€æŸ¥ç‚¹
        if ENABLE_CHECKPOINT and gen % CHECKPOINT_EVERY_GEN == 0:
            try:
                # ä¿å­˜åˆ° latestï¼ˆç”¨äºæ¢å¤ï¼‰
                latest_checkpoint = CHECKPOINT_DIR / "checkpoint_latest.json"
                save_checkpoint(
                    latest_checkpoint,
                    gen,
                    population,
                    best_overall_ind,
                    best_overall_fitness,
                    history_qwk,
                    history_pearson,
                    history_rmse,
                    history_llm_stats,
                    _LLM_CACHE,
                )
                
                # åŒæ—¶ä¿å­˜å¸¦ä»£æ•°çš„å¤‡ä»½
                gen_checkpoint = CHECKPOINT_DIR / f"checkpoint_gen_{gen}.json"
                save_checkpoint(
                    gen_checkpoint,
                    gen,
                    population,
                    best_overall_ind,
                    best_overall_fitness,
                    history_qwk,
                    history_pearson,
                    history_rmse,
                    history_llm_stats,
                    _LLM_CACHE,
                )
                
                # æ¸…ç†æ—§æ£€æŸ¥ç‚¹ï¼ˆä¿ç•™æœ€è¿‘ 3 ä¸ªï¼‰
                clean_old_checkpoints(CHECKPOINT_DIR, keep_last=3)
            except Exception as e:
                print(f"âš ï¸  Failed to save checkpoint: {e}")

    print("\n==== Evolution Finished (HF ICL-only) ====")
    if best_overall_ind is None:
        print("ERROR: best_overall_ind is None")
        return

    print(f"ğŸ¯ Best Fitness={best_overall_fitness:.4f}")
    print("Best Genome:", best_overall_ind.genome)

    payload = {
        "best_genome": asdict(best_overall_ind.genome),
        "best_metrics": best_overall_ind.metrics,
        "best_fitness": best_overall_fitness,
        "history_qwk": history_qwk,
        "history_pearson": history_pearson,
        "history_rmse": history_rmse,
        "history_llm_stats": history_llm_stats,
        "single_model": SINGLE_MODEL,
    }

    with open(BEST_JSON, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    best_prompt_text = build_full_prompt(
        genome=best_overall_ind.genome,
        essay="<<ESSAY_PLACEHOLDER>>",
        icl_examples=[],
        rag_examples=[],
        summary_text=None,
    )
    with open(BEST_TXT, "w", encoding="utf-8") as f:
        f.write(best_prompt_text)

    print(f"ğŸ“ Best JSON: {BEST_JSON}")
    print(f"ğŸ“ Best prompt txt: {BEST_TXT}")

    gens = list(range(1, N_GENERATIONS + 1))
    plt.figure(figsize=(8, 5))
    plt.plot(gens, history_qwk, marker="o", label="QWK (â†‘)")
    plt.plot(gens, history_pearson, marker="s", label="Pearson (â†‘)")
    plt.plot(gens, history_rmse, marker="^", label="RMSE (â†“)")
    plt.xlabel("Generation")
    plt.ylabel("Metric Value")
    plt.title("Best Metrics per Generation (HF ICL-only)")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig(METRIC_FIG, dpi=150)
    plt.close()
    print(f"ğŸ“Š Metrics curve saved: {METRIC_FIG}")

    print("\n====== Final Evaluation with Best Genome (CSV) ======")
    final_pool = eval_pool

    preds, raws, labels, essays_out = [], [], [], []

    for i, (sid, essay, true_band, prompt_text) in enumerate(final_pool, start=1):
        # ğŸ”¥ æ ¹æ®æ¨¡å¼é€‰æ‹© ICL ç¤ºä¾‹
        if best_overall_ind.genome.use_icl_indices and best_overall_ind.genome.icl_indices:
            icl_examples = select_icl_examples(
                train_pool,
                strategy="random",
                k=best_overall_ind.genome.k_shots,
                seed=sid,
                indices=best_overall_ind.genome.icl_indices,
            )
        else:
            icl_examples = select_icl_examples(
                train_pool,
                strategy=best_overall_ind.genome.icl_strategy,
                k=best_overall_ind.genome.k_shots,
                seed=sid,
            )

        full_prompt = build_full_prompt(
            genome=best_overall_ind.genome,
            essay=essay,
            icl_examples=icl_examples,
            rag_examples=[],
            summary_text=None,
        )

        if len(full_prompt) > MAX_CONTEXT_CHARS:
            head = full_prompt[: int(MAX_CONTEXT_CHARS * 0.75)]
            tail = full_prompt[-int(MAX_CONTEXT_CHARS * 0.20):]
            full_prompt = head + "\n\n[Truncated]\n\n" + tail

        cache_key = full_prompt
        reply = _LLM_CACHE.get(cache_key)
        if reply is None:
            throttle()
            reply = call_scoring_llm(
                full_prompt,
                temperature=0.0,
                model=SINGLE_MODEL,
                max_tokens=8,
                max_retries=3,
                timeout=90,
            )
            _LLM_CACHE[cache_key] = reply

        raw_band = parse_band_from_text(reply or "", default=5.0)
        pred_band = round(raw_band * 2) / 2.0

        preds.append(pred_band)
        raws.append(raw_band)
        labels.append(true_band)
        essays_out.append(essay[:2000])

        print(
            f"  Essay {i}/{len(final_pool)} | "
            f"True={true_band:.1f}, Raw={raw_band:.1f}, Pred={pred_band:.1f}"
        )

    final_metrics = compute_metrics(labels, preds)
    print(
        f"\nFinal Evaluation â†’ "
        f"QWK={final_metrics['qwk']:.4f}, "
        f"Pearson={final_metrics['pearson']:.4f}, "
        f"RMSE={final_metrics['rmse']:.4f}, "
        f"Exact={final_metrics['exact_acc']:.3f}, "
        f"Adj={final_metrics['adj_acc']:.3f}"
    )

    df_out = pd.DataFrame(
        {
            "essay": essays_out,
            "true_band": labels,
            "raw_band": raws,
            "pred_band": preds,
        }
    )
    df_out.to_csv(BEST_PRED_CSV, index=False, encoding="utf-8-sig")
    print(f"ğŸ“„ Predictions saved: {BEST_PRED_CSV}")
    print("ğŸ‰ Done!")



if __name__ == "__main__":
    run_evolution_hf_icl_only()
